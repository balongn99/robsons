#!/bin/bash
#SBATCH --job-name=kongi
#SBATCH --nodes=1
#SBATCH --ntasks=1            # 1 MPI task (we use Python multiprocessing)
#SBATCH --cpus-per-task=32    # total cores on the node
#SBATCH --mem=100G
#SBATCH --time=168:00:00
#SBATCH --output=kongi_%j.out

# ────────────────────────────────────────────────────────────────
# 0.  Environment
# ────────────────────────────────────────────────────────────────
source /gpfs/helios/home/balong/miniconda3/etc/profile.d/conda.sh
conda activate robsons

export KONGI_LOGDIR=$SLURM_SUBMIT_DIR/kongi_runs         # change if needed
export OMP_NUM_THREADS_PER_WORKER=1                        # xTB threads / opt
export KONGI_WORKERS=$SLURM_CPUS_PER_TASK                 # for legacy script
export OMP_NUM_THREADS=$OMP_NUM_THREADS_PER_WORKER         # hard cap for xTB

# ────────────────────────────────────────────────────────────────
# 1.  Create / update task list
# ────────────────────────────────────────────────────────────────
TASKFILE=$SLURM_SUBMIT_DIR/tasklist.tsv.gz
echo ">> Generating task list at $(date)"
python make_tasklist.py -o "$TASKFILE" --run-dir "$KONGI_LOGDIR"
echo ">> Task list ready: $(zcat "$TASKFILE" | wc -l) lines"

# ────────────────────────────────────────────────────────────────
# 2.  Run *all* tasks on this node (32 cores, 1 OMP thread each)
# ────────────────────────────────────────────────────────────────
echo ">> Starting optimisation pool at $(date)"
TOTAL_LINES=$(zcat "$TASKFILE" | wc -l)
python run_chunk.py "$TASKFILE" 0 "$TOTAL_LINES" --omp "$OMP_NUM_THREADS_PER_WORKER"
echo ">> Finished optimisation pool at $(date)"

# ────────────────────────────────────────────────────────────────
# 3.  Merge JSON → ASE database (optional but handy)
# ────────────────────────────────────────────────────────────────
echo ">> Merging results"
python merge_results.py --run-dir "$KONGI_LOGDIR"
echo ">> Done at $(date)"


